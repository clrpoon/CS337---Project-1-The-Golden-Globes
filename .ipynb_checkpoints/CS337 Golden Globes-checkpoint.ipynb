{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\casey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\casey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\casey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\casey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import operator\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install nltk\n",
    "\n",
    "pip install beautifulsoup4\n",
    "\n",
    "pip install spacy\n",
    "\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gg2020.json', encoding='utf8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "gg_data = []\n",
    "for line in lines:\n",
    "    gg_data.append(json.loads(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_awards = [\"Best Performance by an Actress in a Motion Picture - Drama\", \n",
    "          \"Best Performance by an Actor in a Motion Picture - Drama\", \n",
    "          \"Best Performance by an Actress in a Motion Picture - Musical or Comedy\",\n",
    "         \"Best Performance by an Actor in a Motion Picture - Musical or Comedy\",\n",
    "         \"Best Performance by an Actress in a Supporting Role in any Motion Picture\",\n",
    "         \"Best Performance by an Actor in a Supporting Role in any Motion Picture\",\n",
    "         \"Best Director - Motion Picture\",\n",
    "         \"Best Screenplay - Motion Picture\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5146"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_host = []\n",
    "for tweet in gg_data: \n",
    "    tweet_text = tweet['text']\n",
    "    if(\"host\" in tweet_text.lower()) :\n",
    "        tweets_with_host.append(tweet)\n",
    "len(tweets_with_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(text):\n",
    "    article = nlp(text)\n",
    "    labels = [x.label_ for x in article.ents]\n",
    "    [(x.orth_,x.pos_, x.lemma_) for x in [y \n",
    "                                      for y\n",
    "                                      in nlp(text) \n",
    "                                      if not y.is_stop and y.pos_ != 'PUNCT']]\n",
    "    parts_of_speech = dict([(str(x), x.label_) for x in nlp(text).ents])\n",
    "    names = []\n",
    "    for (key, value) in parts_of_speech.items() :\n",
    "        if(value == \"PERSON\") :\n",
    "            names.append(key)\n",
    "    return names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ricky Gervais\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ricky Gervais'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_host_name():\n",
    "    host_name_count = {}\n",
    "    for tweet in tweets_with_host: \n",
    "        names = get_names(tweet[\"text\"])\n",
    "        for name in names :\n",
    "            if name in host_name_count :\n",
    "                host_name_count[name] = host_name_count[name] + 1\n",
    "            else :\n",
    "                host_name_count[name] = 1\n",
    "    host = max(host_name_count.items(), key=operator.itemgetter(1))[0]\n",
    "    print(host)\n",
    "    return host\n",
    "get_host_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of tweets containting the given string \n",
    "def get_tweets(s):\n",
    "    tweets_with_str = []\n",
    "    for tweet in gg_data: \n",
    "        tweet_text = tweet['text']\n",
    "        if(s.lower() in tweet_text.lower()) :\n",
    "            tweets_with_str.append(tweet)\n",
    "    len(tweets_with_str)\n",
    "    return tweets_with_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter list of tweet bodies for word (string)\n",
    "def filter_tweets(tweets, s):\n",
    "    tweets_with_str = []\n",
    "    for tweet in tweets:\n",
    "        tweet_lower = tweet.lower()\n",
    "        if s in tweet_lower:\n",
    "            tweets_with_str.append(tweet)\n",
    "    return tweets_with_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Best Performance by an Actress in a Motion Picture - Drama': 'Renée Zellweger',\n",
       " 'Best Performance by an Actor in a Motion Picture - Drama': 'Joaquin Phoenix',\n",
       " 'Best Performance by an Actress in a Motion Picture - Musical or Comedy': 'Ana de Armas',\n",
       " 'Best Performance by an Actor in a Motion Picture - Musical or Comedy': 'Leonardo DiCaprio',\n",
       " 'Best Performance by an Actress in a Supporting Role in any Motion Picture': 'Laura Dern',\n",
       " 'Best Performance by an Actor in a Supporting Role in any Motion Picture': 'Brad Pitt',\n",
       " 'Best Director - Motion Picture': 'Sam Mendes',\n",
       " 'Best Screenplay - Motion Picture': 'Quentin Tarantino'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# awards_list is a list of award strings [\"Best Motion Picture - Drama\", \"Best Director\", etc]\n",
    "# assuming winners are the most name dropped with an award, this can also be extended to find nominees\n",
    "# returns map of award : winner (hopefully)\n",
    "def get_award_winners(awards_list):\n",
    "    award_winners = {}\n",
    "    for award in awards_list:\n",
    "        tweets_with_award = get_tweets(award)\n",
    "        print(tweets_with_award)\n",
    "        name_count = {}\n",
    "        for tweet in tweets_with_award: \n",
    "            names = get_names(tweet[\"text\"])\n",
    "            for name in names :\n",
    "                if name.lower() == \"goldenglobes\" or name.lower() == \"golden globes\" or len(name.split()) < 2:\n",
    "                    pass\n",
    "                elif name in name_count :\n",
    "                    name_count[name] = name_count[name] + 1\n",
    "                else :\n",
    "                    name_count[name] = 1\n",
    "        winner = max(name_count.items(), key=operator.itemgetter(1))[0]\n",
    "        award_winners[award] = winner\n",
    "    return award_winners\n",
    "\n",
    "get_award_winners(people_awards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_award_presenters(awards_list):\n",
    "    award_presenters = {}\n",
    "    for award in awards_list:\n",
    "        tweets_with_award = get_tweets(award)\n",
    "        tweets_with_award_presenters = filter_tweets(tweets_with_award, 'host')\n",
    "        name_count = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Awkwafina': 'PERSON', 'last night': 'TIME', 'Farewell': 'ORG'}\n"
     ]
    }
   ],
   "source": [
    "text = gg_data[610]['text']\n",
    "article = nlp(text)\n",
    "labels = [x.label_ for x in article.ents]\n",
    "[(x.orth_,x.pos_, x.lemma_) for x in [y \n",
    "                                  for y\n",
    "                                  in nlp(text) \n",
    "                                  if not y.is_stop and y.pos_ != 'PUNCT']]\n",
    "parts_of_speech = dict([(str(x), x.label_) for x in nlp(text).ents])\n",
    "print(parts_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  •/JJ\n",
      "  (ORGANIZATION Priyanka/NNP Chopra/NNP)\n",
      "  and/CC\n",
      "  (PERSON Nick/NNP Jonas/NNP)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  77th/CD\n",
      "  (PERSON Annual/NNP Golden/NNP Globe/NNP Awards/NNP)\n",
      "  present/VBD\n",
      "  the/DT\n",
      "  award/NN\n",
      "  for/IN\n",
      "  (GPE Best/NNP)\n",
      "  TV/NN\n",
      "  series/NN\n",
      "  for/IN\n",
      "  a/DT\n",
      "  musical/JJ\n",
      "  or/CC\n",
      "  comedy/NN\n",
      "  •/JJ\n",
      "  #/#\n",
      "  goldenglobes2020/JJ\n",
      "  @/NNP\n",
      "  (PERSON Golden/NNP Globes/NNP)\n",
      "  https/NN\n",
      "  :/:\n",
      "  //t.co/fivnKsE9Ib/NN)\n"
     ]
    }
   ],
   "source": [
    "ne_tree = ne_chunk(pos_tag(word_tokenize(gg_data[0][\"text\"])))\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('•', 'JJ'),\n",
       " ('Priyanka', 'NNP'),\n",
       " ('Chopra', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Nick', 'NNP'),\n",
       " ('Jonas', 'NNP'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('77th', 'CD'),\n",
       " ('Annual', 'NNP'),\n",
       " ('Golden', 'NNP'),\n",
       " ('Globe', 'NNP'),\n",
       " ('Awards', 'NNP'),\n",
       " ('present', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('award', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('Best', 'NNP'),\n",
       " ('TV', 'NN'),\n",
       " ('series', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('musical', 'JJ'),\n",
       " ('or', 'CC'),\n",
       " ('comedy', 'NN'),\n",
       " ('•', 'JJ'),\n",
       " ('#', '#'),\n",
       " ('goldenglobes2020', 'JJ'),\n",
       " ('@', 'NNP'),\n",
       " ('Golden', 'NNP'),\n",
       " ('Globes', 'NNP'),\n",
       " ('https', 'NN'),\n",
       " (':', ':'),\n",
       " ('//t.co/fivnKsE9Ib', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = preprocess(gg_data[0][\"text\"])\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  •/JJ\n",
      "  Priyanka/NNP\n",
      "  Chopra/NNP\n",
      "  and/CC\n",
      "  Nick/NNP\n",
      "  Jonas/NNP\n",
      "  at/IN\n",
      "  the/DT\n",
      "  77th/CD\n",
      "  Annual/NNP\n",
      "  Golden/NNP\n",
      "  Globe/NNP\n",
      "  Awards/NNP\n",
      "  present/VBD\n",
      "  (NP the/DT award/NN)\n",
      "  for/IN\n",
      "  Best/NNP\n",
      "  (NP TV/NN)\n",
      "  (NP series/NN)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  musical/JJ\n",
      "  or/CC\n",
      "  (NP comedy/NN)\n",
      "  •/JJ\n",
      "  #/#\n",
      "  goldenglobes2020/JJ\n",
      "  @/NNP\n",
      "  Golden/NNP\n",
      "  Globes/NNP\n",
      "  (NP https/NN)\n",
      "  :/:\n",
      "  (NP //t.co/fivnKsE9Ib/NN))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-14c1f7594735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgg_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "doc = nlp(gg_data[0][\"text\"])\n",
    "pprint([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_string(url):\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tweet = gg_data[0][\"text\"]\n",
    "article = nlp(first_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [x.label_ for x in article.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x.orth_,x.pos_, x.lemma_) for x in [y \n",
    "                                      for y\n",
    "                                      in nlp(first_tweet) \n",
    "                                      if not y.is_stop and y.pos_ != 'PUNCT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict([(str(x), x.label_) for x in nlp(first_tweet).ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
